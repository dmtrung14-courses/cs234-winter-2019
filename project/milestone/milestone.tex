%%%%%%%% ICML 2018 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2018} with \usepackage[nohyperref]{icml2018} above.
\usepackage{hyperref}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2018}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2018}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Project Milestone for CS234 (Winter 2019)}




\usepackage{amsmath, amsthm, amssymb}



\newcommand{\bvec}[1]{\boldsymbol{#1}}


\begin{document}

\twocolumn[
\icmltitle{Project Milestone for CS234 (Winter 2019) \\
			Generalized Locomotion Controllers}

% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Li Quan Khoo}{stanford,scpd}
\end{icmlauthorlist}

\icmlaffiliation{stanford}{Computer Science Department}
\icmlaffiliation{scpd}{Stanford Center for Professional Development}

\icmlcorrespondingauthor{Li Quan Khoo}{lqkhoo@stanford.edu, li.khoo.11@ucl.ac.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
] % \twocolumn

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\section{Change of project}
First, we need to address the reason for the deviation of the final project topic from the initial project proposal. The original proposal was about reinforcement learning (RL) under incomplete information on the board game Stratego. For more detailed information about Stratego, please refer to the initial project proposal, as well as official competition rules \cite{ISFrules}. 

In short, there are several practical reasons behind this change. Also, after developing a slightly more mature understanding of RL, we decided to tackle a problem that is more relevant, and more broadly applicable.
\begin{itemize}
	\item Stratego is an incomplete information game. However, under the RL framework, this just means the policy acts on observations rather than states, i.e. $\pi(s,a)$ becomes $\pi(o,a)$. There is only a small (immutable) set of pieces in Stratego, and we think the novelty of the problem is diminished under this perspective. Under the intended representation of the game state, the agent has access to information about its own pieces, the opponent's known and unknown pieces.
	\item There is a notable lack of recorded Stratego games. AlphaGo was bootstrapped initially by imitating expert play.
	\item The state-space of Stratego is $10^{115}$, compared to chess's $10^{45}$. For a one-man project, even if this problem is solvable in the allotted time, handling this complexity should not be the focus of a project of this scope. At least, this was not one of the goals in the original proposal, especially since the class will not be covering MCTS methods for exploring this state space until towards the end.
	\item Stratego as a game is not interesting in the sense that it doesn't act as any sort of benchmark for RL. For example, Go was novel because of its enormous state space ($10^{360}$), and Montezuma's Revenge is an important gauge for exploration. We find it difficult to see how the results would inform us about RL other than (possibly) solving the game itself. Even then there is no 'solution' such as in draughts in the traditional sense since the game is stochastic.
	\item Due to the lack of an existing implementation of Stratego that exposes an interface that allows for automated policy training etc., it is necessary to implement one (or hack one). After reading through competition rules, which is important for understanding the necessary representation of the state of the game (e.g. The state of the game in Go requires knowing the past 7 moves). Our conclusion is that even a simplified implementation is not the best use of time for this project, which should really be spent exploring RL itself.
\end{itemize}

\section{Background}
\subsection{Locomotion policies under pure RL}
For locomotion tasks trained entirely using RL techniques, we tend to notice several things that could be improved. In particular, training the policy can be highly sample-inefficient due to lack of domain knowledge. There is also a certain unnaturalness or inefficiency of gait due to loose constraints (e.g. flailing arms in a running humanoid), jerkiness of motion due to discontinuous policy or feedback, in continuous state and action spaces.

In the scope of rigid bodies interacting with the environment, locomotion can only happen via surface-to-surface contacts between the agent and the environment. These are temporary degrees of constraints (DOCs) depending on the nature of the contact (e.g. sliding, rolling, fixed), which are otherwise identical to joint articulations between rigid bodies. Although MuJoCo provides a highly simplified model of Newtonian fluids, for the sake of brevity let's set that aside for now.

The fact is we are dealing with a physical system, and for performance reasons we should really be treating it as one. For instance, in \cite{heess2017emergence} [\href{https://www.youtube.com/watch?v=hx_bgoTF7bs}{video}], locomotion behaviours are synthesized from simple reward functions in humanoid models.

The result is certainly impressive in at least two ways: The RL algorithm they used needed to know next to nothing about what it is actually controlling to generate a usable policy, and the learned policy "generalizes" to different tasks (the terrain and obstacles). However, reinforcement learning is most relevant when we \textit{really} cannot know the dynamics of a system, or the solution is intractable for a control system, e.g. in fluid dynamics \cite{mit6832}. For rigid-body systems, we can certainly do a much better job in designing a locomotion controller, and we can start by including physical information into our model. This brings us to control theory.

\subsection{Control theory}
The field of control theory is specifically developed to deal with dynamical systems in continuous state spaces. A simple example of a linear feedback control system is the PID (proportional-integral-derivative) controller, whose control function (the response) is defined as:
\begin{align*}
\bvec{y}(t) &= w_1 \bvec{x}(t) + w_2\int_{t_1}^{t_2} \bvec{x}(t') dt' + w_3 \frac{d\bvec{x}}{dt}
\end{align*}

where $w$ terms are tunable scalar parameters, and $\bvec{x}$ is the state vector of the system, e.g. 6 degrees of motion of a rigid body, or temperature etc.

This could be understood as a controller whose response is linear with respect to the actual value of a variable, its instantaneous time delta, as well as moving average (whose job is to correct for a so-called steady-state error). It is notably not optimal for nonlinear systems.

More generally, in optimal control theory, in the special case where system dynamics could be described by a set of \textit{linear} differential equations, and where the cost is described by a \textit{quadratic} function, we have a so-called linear-quadratic (LQ) problem, and the optimal control solution is called the linear-quadratic regulator (LQR), which can be found by solving the system's Ricatti equation. In the related LQG (linear-quadratic-Gaussian) regime where both our readings of $\bvec{x}(t)$ and outputs $\bvec{y}(t)$ are corrupted by additive white Gaussian noise, the optimal solution is comprised of the gains from a Kalman filter on $\bvec{x}$ applied on top of the associated LQR.

\subsection{Other control methods}
There are other control methods for dynamic processes, such as model predictive control (MPC). These generally require a model of the specific process we are concerned about optimizing, and are thus not generalizable to different tasks. We shall not describe them here.

\section{Introduction}
The method which we are going to be exploring shall attempt to combine the best of both worlds: the efficient trajectory search from trajectory optimization methods (via LQR or otherwise), and policy tuning via reinforcement learning, which provides the generality over very different trajectories. Emphasis: this is not an original idea. In fact, this proposal is primarily inspired by the following two papers:

\begin{itemize}
	\item Interactive Control of Diverse Complex Characters with Neural Networks \cite{mordatch2015interactive} [\href{https://www.youtube.com/watch?v=57D-qgVX-6o}{video}]
	\item Discovery of Complex Behaviors through Contact-Invariant Optimization \cite{mordatch2012discovery} [\href{https://www.youtube.com/watch?v=mhr_jtQrhVA}{video}]
\end{itemize}

Since it is not a requirement of the project to develop something novel, we are treating this as an opportunity to implement a solution which is known to work, as well as taking the opportunity to dig deeper into trajectory optimization and control theory, which we (I) am practically scrambling to understand from scratch, in order to make sense of the literature as well as to push the project forward.

Compared to the original Stratego proposal, this line of inquiry allows time to become more familiar with MuJoCo itself, and provides a stepping stone for further research in locomotion. For instance, the authors have applied a similar method in \cite{mordatch2012contact} to achieve high-dexterity control of a robotic hand.

We shall use the remaining space in the 3-page limit to describe the most important ideas of the approach, before going into what we expect to be able to do in the coming month, which is the limiting factor for scoping what we are able to do.

Just to clarify that this is a project about reinforcement learning, the method the authors used to produce the final policy is via block-alternating optimization, where they optimize, in turn, the trajectory (by penalizing deviations from the policy), and then the policy (by minimizing the loss w.r.t a set of trajectories, for different tasks). In this sense, we do not initially know what the solution is - trajectory optimization solves each task individually for us - and then the policy is trained to imitate the trajectories generated for \textit{all} tasks at once in order to generalize to unseen states and (and hopefully tasks).

\subsection{Additive noise}
In \cite{mordatch2015interactive}, citing other prior work, states that injecting noise during training makes the resulting policy more robust, and noise is added to prevent divergent policies during execution. They add independent Gaussian noise $\epsilon \sim \mathcal{N}(\bvec{0}, \bvec{\sigma}^2_{\epsilon}\bvec{I})$ to all sensory inputs, and showed that if the noise is small enough, the optimal action at nearby noisy states is given by the first-order Taylor approximation $\bvec{a}(\bvec{s}+\epsilon) \approx \bvec{a}(s) + \frac{d\bvec{a}}{d\bvec{s}}\epsilon$.

In addition, they added Gaussian noise $\gamma \sim \mathcal{N}(\bvec{0}, \bvec{\sigma}^2_{\gamma}\bvec{I})$ to the outputs of their hidden layers, as a network regularizer, in order to make the controller less sensitive to specific values of the hidden units.

The authors emphasized (and showed) the importance of this noise in the robustness of their resulting policies.

\subsection{Contact-Invariant Optimization (CIO)}
This is the main idea behind the 2012 paper. CIO optimizes over a composite objective function which is linear across four different loss terms. As defined in \cite{mordatch2012contact}:

\begin{align*}
L(\bvec{s}) &= L_{CI}(\bvec{s}) + L_{Physics}(\bvec{s}) + L_{Task}(\bvec{s}) + L_{Hint}(\bvec{s})
\end{align*}

There are variants of it to account for non-planar contacts \cite{mordatch2012contact}, but the most important feature is its decomposability. CIO optimizes over a the contact-invariant (CI) loss function, which is linear in four component losses, and by disabling or enabling different components at different times, it can speed up the process of trajectory search. As defined in \cite{mordatch2012contact}, CI loss has the following definition:

todo

\section{Approach}

todo


\bibliography{milestone}
\bibliographystyle{icml2018}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% DELETE THIS PART. DO NOT PLACE CONTENT AFTER THE REFERENCES!
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018. It was modified from a version from Dan Roy in
% 2017, which was based on a version from Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
